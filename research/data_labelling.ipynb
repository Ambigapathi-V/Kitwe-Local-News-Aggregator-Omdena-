{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ambig\\\\jupiter_notebook\\\\Projects\\\\Kitwe-Local-News-Aggregator-Omdena-'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\ambig\\jupiter_notebook\\Projects\\Kitwe-Local-News-Aggregator-Omdena-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ambig\\\\jupiter_notebook\\\\Projects\\\\Kitwe-Local-News-Aggregator-Omdena-'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class DataLabellingConfig:\n",
    "    root_dir:Path\n",
    "    input_path : Path\n",
    "    output_path: Path\n",
    "    reputable_sources: List[str]\n",
    "    suspicious_domain_patterns: List[str]\n",
    "    sensational_keywords: List[str]\n",
    "    thresholds: Dict[str, float]\n",
    "    model_path : Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.newsaggregator.constants import *\n",
    "from src.newsaggregator.utils.common import read_yaml , create_directories\n",
    "from src.newsaggregator import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, \n",
    "                 params_filepath=PARAMS_FILE_PATH, \n",
    "                 schema_filepath=SCHEMA_FILE_PATH\n",
    "                 ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_data_labelling_config(self) -> DataLabellingConfig:\n",
    "        config = self.config.data_labelling\n",
    "        create_directories([config['root_dir']])\n",
    "        \n",
    "        data_labelling_config = DataLabellingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            input_path=Path(config['input_path']),\n",
    "            output_path=Path(config['output_path']),\n",
    "            model_path = Path(config['model_path']),\n",
    "            reputable_sources=config['reputable_sources'],\n",
    "            suspicious_domain_patterns=config['suspicious_domain_patterns'],\n",
    "            sensational_keywords=config['sensational_keywords'],\n",
    "            thresholds=config['thresholds']\n",
    "        )\n",
    "        return data_labelling_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLabelling:\n",
    "    def __init__(self, config):\n",
    "        # Load the input data and assign it to the `data` attribute\n",
    "        self.data = pd.read_csv(config.input_path)\n",
    "        self.reputable_sources = config.reputable_sources\n",
    "        self.suspicious_domain_patterns = re.compile(config.suspicious_domain_patterns[0])\n",
    "        self.sensational_keywords = config.sensational_keywords\n",
    "        self.thresholds = config.thresholds\n",
    "        self.output_path = config.output_path\n",
    "\n",
    "        # Fill NaN values in important columns to avoid issues in processing\n",
    "        self.data['Description'] = self.data['Description'].fillna(\"\")\n",
    "        self.data['Headline'] = self.data['Headline'].fillna(\"\")\n",
    "        self.data['Author'] = self.data['Author'].fillna(\"\")\n",
    "\n",
    "        # Log successful initialization\n",
    "        logger.info(\"DataLabelling class initialized with input data and configuration.\")\n",
    "        logger.debug(f\"Input data shape: {self.data.shape}\")\n",
    "\n",
    "    def check_source_credibility(self, url):\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc.lower()\n",
    "        credibility = 0  # Default to neutral\n",
    "        \n",
    "        if any(source in domain for source in self.reputable_sources):\n",
    "            credibility = -1  # Reputable source\n",
    "            logger.debug(f\"URL '{url}' found to be a reputable source.\")\n",
    "        elif self.suspicious_domain_patterns.search(domain):\n",
    "            credibility = 1  # Suspicious source\n",
    "            logger.debug(f\"URL '{url}' found to be suspicious.\")\n",
    "        \n",
    "        return credibility\n",
    "\n",
    "    def detect_clickbait(self, headline):\n",
    "        if not isinstance(headline, str):\n",
    "            logger.debug(\"Headline is not a string.\")\n",
    "            return 0\n",
    "        excessive_punctuation = len(re.findall(r'[!?.]{2,}', headline)) > 0\n",
    "        all_caps = headline.isupper()\n",
    "        provocative_words = any(word in headline.lower() for word in [\n",
    "            'shocking', 'unbelievable', \"you wont believe\", 'secret', \n",
    "            'amazing', 'incredible'\n",
    "        ])\n",
    "        if excessive_punctuation or all_caps or provocative_words:\n",
    "            logger.debug(f\"Clickbait detected in headline: {headline}\")\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def count_sensational_keywords(self, description):\n",
    "        if not isinstance(description, str):\n",
    "            logger.debug(\"Description is not a string.\")\n",
    "            return 0\n",
    "        count = sum(description.lower().count(word) for word in self.sensational_keywords)\n",
    "        logger.debug(f\"Sensational keywords counted: {count} in description.\")\n",
    "        return count\n",
    "\n",
    "    def apply_topic_modeling(self):\n",
    "        logger.info(\"Applying topic modeling...\")\n",
    "        count_vectorizer = CountVectorizer(max_features=300, stop_words='english')\n",
    "        count_data = count_vectorizer.fit_transform(self.data['Description'].astype(str))\n",
    "        \n",
    "        lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "        lda.fit(count_data)\n",
    "        topic_distribution = lda.transform(count_data)\n",
    "        \n",
    "        dominant_topics = topic_distribution.argmax(axis=1)\n",
    "        logger.debug(f\"Dominant topics assigned: {dominant_topics}\")\n",
    "        return dominant_topics\n",
    "\n",
    "    def get_sentiment_score(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            logger.debug(\"Sentiment text is not a string.\")\n",
    "            return 0\n",
    "        try:\n",
    "            sentiment = TextBlob(text).sentiment\n",
    "            logger.debug(f\"Sentiment score calculated: {sentiment.polarity} for text: {text}\")\n",
    "            return sentiment.polarity\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating sentiment: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def check_mismatch_headline_description(self, row):\n",
    "        headline, description = row['Headline'], row['Description']\n",
    "        if not (isinstance(headline, str) and isinstance(description, str)):\n",
    "            logger.debug(\"Headline or description is not a string.\")\n",
    "            return 0\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        combined_text = [headline, description]\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(combined_text)\n",
    "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "        mismatch = similarity[0][0] < 0.3\n",
    "        if mismatch:\n",
    "            logger.debug(f\"Mismatch detected between headline and description: {headline} | {description}\")\n",
    "        return mismatch\n",
    "\n",
    "    def check_excessive_capitalization(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            logger.debug(\"Text is not a string.\")\n",
    "            return 0\n",
    "        words = text.split()\n",
    "        capitalized_words = [word for word in words if word.isupper() and len(word) > 1]\n",
    "        excessive = len(capitalized_words) > self.thresholds['excessive_capitalization']\n",
    "        if excessive:\n",
    "            logger.debug(f\"Excessive capitalization detected in text: {text}\")\n",
    "        return excessive\n",
    "\n",
    "    def check_vague_author(self, author):\n",
    "        if not isinstance(author, str):\n",
    "            logger.debug(\"Author is not a string.\")\n",
    "            return 0\n",
    "        vague_authors = ['admin', 'editor', 'newsroom', 'staff', 'unknown']\n",
    "        is_vague = any(vague_name in author.lower() for vague_name in vague_authors)\n",
    "        if is_vague:\n",
    "            logger.debug(f\"Vague author detected: {author}\")\n",
    "        return is_vague\n",
    "\n",
    "    def count_suspicious_links(self, description):\n",
    "        if not isinstance(description, str):\n",
    "            logger.debug(\"Description is not a string.\")\n",
    "            return 0\n",
    "        urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\\\\\(\\\\\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', description)\n",
    "        count = len(urls)\n",
    "        logger.debug(f\"Suspicious links counted: {count} in description.\")\n",
    "        return count\n",
    "\n",
    "    def check_short_sensational_description(self, description):\n",
    "        if not isinstance(description, str):\n",
    "            logger.debug(\"Description is not a string.\")\n",
    "            return 0\n",
    "        description_length = len(description)\n",
    "        sensational_word_count = self.count_sensational_keywords(description)\n",
    "        if description_length < 100 and sensational_word_count > 1:\n",
    "            logger.debug(f\"Short sensational description detected: {description}\")\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def determine_fake_news(self):\n",
    "        logger.info(\"Determining fake news labels...\")\n",
    "        # Compute indicators and save directly to columns in `self.data`\n",
    "        self.data['Source_Credibility'] = self.data['Link'].apply(self.check_source_credibility)\n",
    "        self.data['Headline_Type'] = self.data['Headline'].apply(self.detect_clickbait)\n",
    "        self.data['Sensational_Keyword_Count'] = self.data['Description'].apply(self.count_sensational_keywords)\n",
    "        self.data['Dominant_Topic'] = self.apply_topic_modeling()\n",
    "        self.data['Sentiment_Score'] = self.data['Description'].apply(self.get_sentiment_score)\n",
    "        self.data['Excessive_Capitalization'] = self.data['Headline'].apply(self.check_excessive_capitalization)\n",
    "        self.data['Headline_Description_Mismatch'] = self.data.apply(self.check_mismatch_headline_description, axis=1)\n",
    "        self.data['Vague_Author'] = self.data['Author'].apply(self.check_vague_author)\n",
    "        self.data['Suspicious_Links_Count'] = self.data['Description'].apply(self.count_suspicious_links)\n",
    "        self.data['Short_Sensational_Description'] = self.data['Description'].apply(self.check_short_sensational_description)\n",
    "\n",
    "        # Logging output to examine the distribution of the indicators\n",
    "        logger.info(\"Indicators computed successfully. Summary statistics:\")\n",
    "        logger.info(self.data[['Source_Credibility', 'Headline_Type', 'Sensational_Keyword_Count', \n",
    "                               'Dominant_Topic', 'Sentiment_Score', 'Excessive_Capitalization', \n",
    "                               'Headline_Description_Mismatch', 'Vague_Author', 'Suspicious_Links_Count', \n",
    "                               'Short_Sensational_Description']].describe())\n",
    "\n",
    "        # Use `apply` to consolidate indicators into 'Target_final'\n",
    "        self.data['Target_final'] = self.data.apply(\n",
    "            lambda row: self.enhanced_determine_fake_news(\n",
    "                row,\n",
    "                row['Source_Credibility'],\n",
    "                row['Headline_Type'],\n",
    "                row['Sensational_Keyword_Count'],\n",
    "                row['Dominant_Topic'],\n",
    "                row['Sentiment_Score'],\n",
    "                row['Excessive_Capitalization'],\n",
    "                row['Headline_Description_Mismatch'],\n",
    "                row['Vague_Author'],\n",
    "                row['Suspicious_Links_Count'],\n",
    "                row['Short_Sensational_Description']\n",
    "            ), axis=1\n",
    "        )\n",
    "\n",
    "        logger.info(\"Target_final label distribution:\\n%s\", self.data['Target_final'].value_counts())\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def enhanced_determine_fake_news(\n",
    "        self, row, source_credibility, headline_type, sensational_keyword_count,\n",
    "        dominant_topic, sentiment_score, excessive_capitalization, \n",
    "        headline_description_mismatch, vague_author, suspicious_links_count, \n",
    "        short_sensational_description\n",
    "    ):\n",
    "        logger.debug(f\"Evaluating row: {row['Headline']} | {row['Description']}\")\n",
    "        \n",
    "        # Decision logic based on the computed indicators\n",
    "        if source_credibility == 1 or headline_type == 1:\n",
    "            logger.debug(\"Marking as fake due to suspicious source or clickbait.\")\n",
    "            return 1\n",
    "        if sensational_keyword_count > 2:\n",
    "            logger.debug(\"Marking as fake due to excessive sensational keywords.\")\n",
    "            return 1\n",
    "        if dominant_topic == 4:  # Assuming topic 4 is fake news-related\n",
    "            logger.debug(\"Marking as fake due to dominant topic.\")\n",
    "            return 1\n",
    "        if sentiment_score < 0:\n",
    "            logger.debug(\"Marking as fake due to negative sentiment.\")\n",
    "            return 1\n",
    "        if excessive_capitalization:\n",
    "            logger.debug(\"Marking as fake due to excessive capitalization.\")\n",
    "            return 1\n",
    "        if headline_description_mismatch:\n",
    "            logger.debug(\"Marking as fake due to headline-description mismatch.\")\n",
    "            return 1\n",
    "        if vague_author:\n",
    "            logger.debug(\"Marking as fake due to vague author.\")\n",
    "            return 1\n",
    "        if suspicious_links_count > 1:\n",
    "            logger.debug(\"Marking as fake due to suspicious links.\")\n",
    "            return 1\n",
    "        if short_sensational_description:\n",
    "            logger.debug(\"Marking as fake due to short sensational description.\")\n",
    "            return 1\n",
    "        \n",
    "        logger.debug(\"Marking as real news.\")\n",
    "        return 0\n",
    "\n",
    "    def save_to_csv(self):\n",
    "        logger.info(\"Saving labeled data to CSV.\")\n",
    "        self.data.to_csv(self.output_path, index=False)\n",
    "        logger.info(f\"Labeled data saved to {self.output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 14:01:00,817: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-30 14:01:00,822: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-30 14:01:00,827: INFO: common: yaml file: schema.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 14:01:00,830: INFO: common: created directory at: artifacts]\n",
      "[2024-10-30 14:01:00,830: INFO: common: created directory at: artifacts/Data_Labelling]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'artifacts\\\\Data_Categorizer\\\\News_Aggregator_labelled.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m config\u001b[38;5;241m=\u001b[39mConfigurationManager()\n\u001b[0;32m      4\u001b[0m data_labelling_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_data_labelling_config()\n\u001b[1;32m----> 5\u001b[0m data_labelling \u001b[38;5;241m=\u001b[39m \u001b[43mDataLabelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_labelling_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Determine fake news\u001b[39;00m\n\u001b[0;32m      8\u001b[0m results \u001b[38;5;241m=\u001b[39m data_labelling\u001b[38;5;241m.\u001b[39mdetermine_fake_news()\n",
      "Cell \u001b[1;32mIn[131], line 4\u001b[0m, in \u001b[0;36mDataLabelling.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Load the input data and assign it to the `data` attribute\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreputable_sources \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mreputable_sources\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuspicious_domain_patterns \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(config\u001b[38;5;241m.\u001b[39msuspicious_domain_patterns[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\ambig\\jupiter_notebook\\Projects\\Kitwe-Local-News-Aggregator-Omdena-\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ambig\\jupiter_notebook\\Projects\\Kitwe-Local-News-Aggregator-Omdena-\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\ambig\\jupiter_notebook\\Projects\\Kitwe-Local-News-Aggregator-Omdena-\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ambig\\jupiter_notebook\\Projects\\Kitwe-Local-News-Aggregator-Omdena-\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\ambig\\jupiter_notebook\\Projects\\Kitwe-Local-News-Aggregator-Omdena-\\venv\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'artifacts\\\\Data_Categorizer\\\\News_Aggregator_labelled.csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load configuration\n",
    "    config=ConfigurationManager()\n",
    "    data_labelling_config = config.get_data_labelling_config()\n",
    "    data_labelling = DataLabelling(data_labelling_config)\n",
    "\n",
    "    # Determine fake news\n",
    "    results = data_labelling.determine_fake_news()\n",
    "    results.head()\n",
    "    results.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
