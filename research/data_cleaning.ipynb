{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ambig\\\\jupiter_notebook\\\\Projects\\\\Kitwe-Local-News-Aggregator-Omdena-\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ambig\\\\jupiter_notebook\\\\Projects\\\\Kitwe-Local-News-Aggregator-Omdena-'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCleaningConfig:\n",
    "    root_dir:Path\n",
    "    input_path : Path\n",
    "    output_path: Path\n",
    "    date_column : str\n",
    "    text_columns : List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.newsaggregator.constants import *\n",
    "from src.newsaggregator.utils.common import read_yaml , create_directories\n",
    "from src.newsaggregator import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH,\n",
    "                 schema_filepath = SCHEMA_FILE_PATH\n",
    "                 ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_data_ingestion_config(self) -> DataCleaningConfig:\n",
    "        config = self.config.data_cleaning\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_transformation_config = DataCleaningConfig(\n",
    "            root_dir=Path(config['root_dir']), \n",
    "            input_path=Path(config['input_path']),  \n",
    "            output_path=Path(config['output_path']),\n",
    "            date_column = config['date_column'], \n",
    "            text_columns = config['text_columns']\n",
    "        )\n",
    "        return data_transformation_config\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaning:\n",
    "    def __init__(self, config: DataCleaningConfig):\n",
    "        self.input_path = config.input_path\n",
    "        self.output_path = config.output_path\n",
    "        self.date_column = config.date_column\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.df = None\n",
    "        self.text_columns = config.text_columns\n",
    "\n",
    "    def load_cleaning(self):\n",
    "        \"\"\"Load the dataset and perform initial cleaning steps.\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.input_path)  # Load the data\n",
    "            logger.info(\"........Loaded Dataset\")\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found at {self.input_path}\")\n",
    "            return\n",
    "        except pd.errors.EmptyDataError:\n",
    "            logger.error(\"No data found in the file.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while loading the dataset: {str(e)}\")\n",
    "            return\n",
    "        \n",
    "        if self.df.empty:\n",
    "            logger.error(\"Loaded DataFrame is empty. Cleaning process aborted.\")\n",
    "            return\n",
    "\n",
    "        logger.info(\"Converting date column into Datetime format\")\n",
    "        if self.date_column in self.df.columns:\n",
    "            self.df[self.date_column] = pd.to_datetime(self.df[self.date_column], errors=\"coerce\")\n",
    "            logger.info(\"Date column converted into Datetime\")\n",
    "        else:\n",
    "            logger.warning(f\"Date column '{self.date_column}' not found in DataFrame.\")\n",
    "\n",
    "        logger.info(\"Dropping duplicates based on 'title' column\")\n",
    "        initial_count = self.df.shape[0]\n",
    "        self.df.drop_duplicates(inplace=True)\n",
    "        final_count = self.df.shape[0]\n",
    "        duplicates_dropped = initial_count - final_count\n",
    "        logger.info(f\"{duplicates_dropped} duplicates dropped successfully\")\n",
    "\n",
    "        self.df.fillna(\"Unknown\", inplace=True)\n",
    "        logger.info(\"Filled missing values as 'Unknown'.\")\n",
    "\n",
    "    def create_output_directory(self):\n",
    "        \"\"\"Create the output directory if it does not exist.\"\"\"\n",
    "        if not os.path.exists(os.path.dirname(self.output_path)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(self.output_path))\n",
    "                logger.info(f\"Output directory created at {os.path.dirname(self.output_path)}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error creating output directory: {e}\")\n",
    "\n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize text by removing punctuation and converting to lowercase.\"\"\"\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = \"\".join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "        return text\n",
    "\n",
    "    def lemmatize_text(self, text: str) -> str:\n",
    "        \"\"\"Lemmatize the input text.\"\"\"\n",
    "        return \" \".join([self.lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    def remove_stopwords(self, text: str) -> str:\n",
    "        \"\"\"Remove stopwords from the text.\"\"\"\n",
    "        return \" \".join([word for word in text.split() if word not in self.stop_words])\n",
    "\n",
    "    def text_preprocessing(self):\n",
    "        \"\"\"Apply text preprocessing to relevant columns.\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.error(\"DataFrame is not loaded. Please run load_cleaning() first.\")\n",
    "            return\n",
    "        \n",
    "        logger.info(\"Starting text preprocessing...\")\n",
    "        for column in self.text_columns:\n",
    "            if column in self.df.columns:\n",
    "                try:\n",
    "                    self.df[column] = self.df[column].astype(str).apply(self.normalize_text)\n",
    "                    self.df[column] = self.df[column].apply(self.lemmatize_text)\n",
    "                    self.df[column] = self.df[column].apply(self.remove_stopwords)\n",
    "                    logger.info(f\"Text preprocessing completed for column: {column}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error occurred while processing column {column}: {str(e)}\")\n",
    "                    raise\n",
    "\n",
    "    def save_cleaned_data(self):\n",
    "        \"\"\"Save the cleaned dataset to a CSV file.\"\"\"\n",
    "        if self.df is not None:\n",
    "            try:\n",
    "                # Create the output directory if it doesn't exist\n",
    "                if not os.path.exists(os.path.dirname(self.output_path)):\n",
    "                    os.makedirs(os.path.dirname(self.output_path))\n",
    "                    logger.info(f\"Output directory created at {os.path.dirname(self.output_path)}\")\n",
    "\n",
    "                logger.info(f\"Saving cleaned data to {self.output_path}...\")\n",
    "                self.df.to_csv(self.output_path, index=False)\n",
    "                logger.info(f\"Cleaned data saved to {self.output_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving cleaned data: {e}\")\n",
    "        else:\n",
    "            logger.error(\"Error: No data available to save.\")\n",
    "\n",
    "    def get_cleaned_data(self):\n",
    "        \"\"\"Returns the cleaned DataFrame.\"\"\"\n",
    "        return self.df\n",
    "\n",
    "    def clean(self):\n",
    "        \"\"\"Main function to execute the entire cleaning process.\"\"\"\n",
    "        logger.info(\"Starting the cleaning process...\")\n",
    "        self.load_cleaning()\n",
    "\n",
    "        if self.df is None or self.df.empty:\n",
    "            logger.error(\"Error: No data loaded. Cleaning process aborted.\")\n",
    "            return\n",
    "\n",
    "        self.text_preprocessing()\n",
    "        self.save_cleaned_data()\n",
    "        logger.info(\"Cleaning process completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-29 12:13:11,194: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-29 12:13:11,194: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-29 12:13:11,194: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-10-29 12:13:11,200: INFO: common: created directory at: artifacts]\n",
      "[2024-10-29 12:13:11,200: INFO: common: created directory at: artifacts/data_cleaning]\n",
      "[2024-10-29 12:13:11,200: INFO: 2996330304: Starting the cleaning process...]\n",
      "[2024-10-29 12:13:11,295: INFO: 2996330304: ........Loaded Dataset]\n",
      "[2024-10-29 12:13:11,295: INFO: 2996330304: Converting date column into Datetime format]\n",
      "[2024-10-29 12:13:11,346: INFO: 2996330304: Date column converted into Datetime]\n",
      "[2024-10-29 12:13:11,346: INFO: 2996330304: Dropping duplicates based on 'title' column]\n",
      "[2024-10-29 12:13:11,363: INFO: 2996330304: 1998 duplicates dropped successfully]\n",
      "[2024-10-29 12:13:11,363: INFO: 2996330304: Filled missing values as 'Unknown'.]\n",
      "[2024-10-29 12:13:11,377: INFO: 2996330304: Starting text preprocessing...]\n",
      "[2024-10-29 12:13:11,486: INFO: 2996330304: Text preprocessing completed for column: Source]\n",
      "[2024-10-29 12:13:11,677: INFO: 2996330304: Text preprocessing completed for column: Category]\n",
      "[2024-10-29 12:13:12,067: INFO: 2996330304: Text preprocessing completed for column: Headline]\n",
      "[2024-10-29 12:13:15,012: INFO: 2996330304: Text preprocessing completed for column: Description]\n",
      "[2024-10-29 12:13:15,012: INFO: 2996330304: Output directory created at artifacts\\data_cleaning\\data]\n",
      "[2024-10-29 12:13:15,012: INFO: 2996330304: Saving cleaned data to artifacts\\data_cleaning\\data\\News_Aggregator_transformed.csv...]\n",
      "[2024-10-29 12:13:15,178: INFO: 2996330304: Cleaned data saved to artifacts\\data_cleaning\\data\\News_Aggregator_transformed.csv]\n",
      "[2024-10-29 12:13:15,178: INFO: 2996330304: Cleaning process completed successfully.]\n",
      "                 Source       Category  \\\n",
      "0     daily mail zambia  headline news   \n",
      "1     daily mail zambia  headline news   \n",
      "2000           flava fm  uncategorized   \n",
      "2001           flava fm  uncategorized   \n",
      "2002           flava fm  uncategorized   \n",
      "\n",
      "                                               Headline  \\\n",
      "0                    k8 million cbu hall building start   \n",
      "1      exercise aimed recruiting 300000 member province   \n",
      "2000  charcoal trader chimwemwe township appeal gove...   \n",
      "2001  ecologist warns urbanisation’s dire impact bio...   \n",
      "2002  copperbelt minister urge mopani settle debt ki...   \n",
      "\n",
      "                                                   Link  \\\n",
      "0     https://www.daily-mail.co.zm/2024/10/15/k8-mil...   \n",
      "1     https://www.daily-mail.co.zm/2024/10/15/exerci...   \n",
      "2000  https://flavaradioandtv.com/charcoal-traders-i...   \n",
      "2001  https://flavaradioandtv.com/ecologist-warns-of...   \n",
      "2002  https://flavaradioandtv.com/copperbelt-ministe...   \n",
      "\n",
      "                                            Description  \\\n",
      "0     mwila ntambi kitwe copperbelt university cbu k...   \n",
      "1     melody mupeta kitwe united party national deve...   \n",
      "2000  pa group charcoal trader cmml area kitwe’s chi...   \n",
      "2001  pecologist mutende simwanza ha raised alarm de...   \n",
      "2002  pcopperbelt minister elisha matambo ha issued ...   \n",
      "\n",
      "                          Date          Author  \n",
      "0    2024-10-15 06:41:52+00:00  Website Editor  \n",
      "1    2024-10-15 06:17:07+00:00  Website Editor  \n",
      "2000 2024-05-13 08:00:53+00:00        Newsroom  \n",
      "2001 2024-03-26 08:33:05+00:00        Newsroom  \n",
      "2002 2024-03-14 10:18:04+00:00        Newsroom  \n"
     ]
    }
   ],
   "source": [
    "config =ConfigurationManager()\n",
    "\n",
    "data_cleaning_config = config.get_data_ingestion_config()\n",
    "\n",
    "data_cleaning = DataCleaning(data_cleaning_config)\n",
    "# Perform the cleaning process\n",
    "data_cleaning.clean()\n",
    "\n",
    "# Optionally retrieve and inspect cleaned data\n",
    "cleaned_data = data_cleaning.get_cleaned_data()\n",
    "if cleaned_data is not None:\n",
    "    print(cleaned_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Category</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Link</th>\n",
       "      <th>Description</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daily Mail Zambia</td>\n",
       "      <td>Headlines, News</td>\n",
       "      <td>K8 million CBU hall building starts</td>\n",
       "      <td>https://www.daily-mail.co.zm/2024/10/15/k8-mil...</td>\n",
       "      <td>MWILA NTAMBI Kitwe COPPERBELT University (CBU)...</td>\n",
       "      <td>Tue, 15 Oct 2024 06:41:52 +0000</td>\n",
       "      <td>Website Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daily Mail Zambia</td>\n",
       "      <td>Headlines, News</td>\n",
       "      <td>Exercise aimed at recruiting 300,000 members i...</td>\n",
       "      <td>https://www.daily-mail.co.zm/2024/10/15/exerci...</td>\n",
       "      <td>MELODY MUPETA Kitwe THE United Party for Natio...</td>\n",
       "      <td>Tue, 15 Oct 2024 06:17:07 +0000</td>\n",
       "      <td>Website Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daily Mail Zambia</td>\n",
       "      <td>Headlines, News</td>\n",
       "      <td>K8 million CBU hall building starts</td>\n",
       "      <td>https://www.daily-mail.co.zm/2024/10/15/k8-mil...</td>\n",
       "      <td>MWILA NTAMBI Kitwe COPPERBELT University (CBU)...</td>\n",
       "      <td>Tue, 15 Oct 2024 06:41:52 +0000</td>\n",
       "      <td>Website Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Daily Mail Zambia</td>\n",
       "      <td>Headlines, News</td>\n",
       "      <td>Exercise aimed at recruiting 300,000 members i...</td>\n",
       "      <td>https://www.daily-mail.co.zm/2024/10/15/exerci...</td>\n",
       "      <td>MELODY MUPETA Kitwe THE United Party for Natio...</td>\n",
       "      <td>Tue, 15 Oct 2024 06:17:07 +0000</td>\n",
       "      <td>Website Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daily Mail Zambia</td>\n",
       "      <td>Headlines, News</td>\n",
       "      <td>K8 million CBU hall building starts</td>\n",
       "      <td>https://www.daily-mail.co.zm/2024/10/15/k8-mil...</td>\n",
       "      <td>MWILA NTAMBI Kitwe COPPERBELT University (CBU)...</td>\n",
       "      <td>Tue, 15 Oct 2024 06:41:52 +0000</td>\n",
       "      <td>Website Editor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Source         Category  \\\n",
       "0  Daily Mail Zambia  Headlines, News   \n",
       "1  Daily Mail Zambia  Headlines, News   \n",
       "2  Daily Mail Zambia  Headlines, News   \n",
       "3  Daily Mail Zambia  Headlines, News   \n",
       "4  Daily Mail Zambia  Headlines, News   \n",
       "\n",
       "                                            Headline  \\\n",
       "0                K8 million CBU hall building starts   \n",
       "1  Exercise aimed at recruiting 300,000 members i...   \n",
       "2                K8 million CBU hall building starts   \n",
       "3  Exercise aimed at recruiting 300,000 members i...   \n",
       "4                K8 million CBU hall building starts   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.daily-mail.co.zm/2024/10/15/k8-mil...   \n",
       "1  https://www.daily-mail.co.zm/2024/10/15/exerci...   \n",
       "2  https://www.daily-mail.co.zm/2024/10/15/k8-mil...   \n",
       "3  https://www.daily-mail.co.zm/2024/10/15/exerci...   \n",
       "4  https://www.daily-mail.co.zm/2024/10/15/k8-mil...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  MWILA NTAMBI Kitwe COPPERBELT University (CBU)...   \n",
       "1  MELODY MUPETA Kitwe THE United Party for Natio...   \n",
       "2  MWILA NTAMBI Kitwe COPPERBELT University (CBU)...   \n",
       "3  MELODY MUPETA Kitwe THE United Party for Natio...   \n",
       "4  MWILA NTAMBI Kitwe COPPERBELT University (CBU)...   \n",
       "\n",
       "                              Date          Author  \n",
       "0  Tue, 15 Oct 2024 06:41:52 +0000  Website Editor  \n",
       "1  Tue, 15 Oct 2024 06:17:07 +0000  Website Editor  \n",
       "2  Tue, 15 Oct 2024 06:41:52 +0000  Website Editor  \n",
       "3  Tue, 15 Oct 2024 06:17:07 +0000  Website Editor  \n",
       "4  Tue, 15 Oct 2024 06:41:52 +0000  Website Editor  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"artifacts\\data_ingestion\\data\\raw\\News_Aggregator_Kitwe_Data_Collection.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
